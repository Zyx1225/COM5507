{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f857e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Define headers for HTTP requests\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0'\n",
    "}\n",
    "\n",
    "def get_page_content(start, max_retries=3, delay=2):\n",
    "    # Construct the URL with the start parameter\n",
    "    url = f'https://www.douban.com/group/search?start={start}&cat=1013&q=%E8%B0%B7%E7%88%B1%E5%87%8C'\n",
    "    \n",
    "    # Set the headers for the request\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0'}\n",
    "\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            # Send an HTTP GET request to the URL\n",
    "            response = requests.get(url, headers=headers, timeout=30)\n",
    "            \n",
    "            # Check if the response was successful\n",
    "            if response.status_code == 200:\n",
    "                # Return the parsed HTML content\n",
    "                return BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        except requests.RequestException:\n",
    "            # Increment the retry counter and wait before retrying\n",
    "            retries += 1\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    # Return None if the request fails after the maximum number of retries\n",
    "    return None\n",
    "\n",
    "def extract_links(soup):\n",
    "    # Find the tbody element that contains the links\n",
    "    tbody = soup.find('tbody')\n",
    "    \n",
    "    links = []\n",
    "    if tbody:\n",
    "        # Find all <a> elements with href attribute inside the tbody\n",
    "        for a in tbody.find_all('a', href=True):\n",
    "            # Append the href value to the links list\n",
    "            links.append(a['href'])\n",
    "    \n",
    "    return links\n",
    "\n",
    "def get_comments(soup):\n",
    "    # Find all <p> elements with the class 'reply-content'\n",
    "    comments = soup.find_all('p', class_='reply-content')\n",
    "    \n",
    "    comment_texts = [comment.get_text(strip=True) for comment in comments]\n",
    "    # Join the comment texts into a single string separated by newlines\n",
    "    comment_text = \"\\n\".join(comment_texts)\n",
    "    \n",
    "    return comment_text\n",
    "\n",
    "def get_context(url, headers, writer):\n",
    "    try:\n",
    "        # Send an HTTP GET request to the URL\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extract the title\n",
    "            title = soup.find('h1').get_text(strip=True)\n",
    "            \n",
    "            # Find the topic content element\n",
    "            topic_content = soup.find(class_='rich-content topic-richtext')\n",
    "            \n",
    "            if topic_content:\n",
    "                # Get the text of the topic content\n",
    "                content_text = topic_content.get_text(strip=True)\n",
    "            else:\n",
    "                content_text = \"\"\n",
    "\n",
    "            # Call get_comments function to extract comments\n",
    "            comment_text = get_comments(soup)\n",
    "\n",
    "            # Write the data to the CSV file\n",
    "            writer.writerow([title, content_text, comment_text])\n",
    "        \n",
    "        else:\n",
    "            print(f\"Failed to retrieve content from {url}\")\n",
    "        \n",
    "        # Add a delay to prevent frequent requests\n",
    "        time.sleep(1)\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        # Handle request exceptions\n",
    "        print(f\"Error occurred while fetching {url}: {e}\")\n",
    "\n",
    "total_pages = 5  # Set the total number of pages to scrape\n",
    "\n",
    "all_links = []\n",
    "\n",
    "# Loop through the range of pages\n",
    "for page in range(1, total_pages + 1):\n",
    "    start = 50 * (page - 1)\n",
    "    \n",
    "    # Get the HTML content of the page\n",
    "    soup = get_page_content(start)\n",
    "    \n",
    "    if soup:\n",
    "        # Extract the links from the page\n",
    "        page_links = extract_links(soup)\n",
    "        \n",
    "        # Add the page links to the all_links list\n",
    "        all_links.extend(page_links)\n",
    "        \n",
    "        # Add a delay to prevent frequent requests\n",
    "        time.sleep(1)\n",
    "    \n",
    "    else:\n",
    "        print(f\"Failed to retrieve content from page {page}\")\n",
    "\n",
    "# Filter out the even-indexed links\n",
    "filtered_links = [link for index, link in enumerate(all_links) if index % 2 == 0]\n",
    "\n",
    "# Create and write the header row to the CSV file\n",
    "with open('Gu Eileen.csv', 'w', newline='', encoding='utf-8-sig') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Content', 'Comment'])\n",
    "\n",
    "# Open the CSV file in append mode and create the writer object\n",
    "with open('Gu Eileen.csv', 'a', newline='', encoding='utf-8-sig') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Loop through the filtered links\n",
    "    for url in filtered_links:\n",
    "        # Call the get_context function to retrieve and write the data\n",
    "        get_context(url, headers, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0682c87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Merging of first acquisition and additional data\n",
    "merged_Gu = pd.DataFrame()\n",
    "csv_files = ['Gu Ailing.csv', 'Gu Ailing1.csv']\n",
    "\n",
    "dfs_to_concat = []\n",
    "\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs_to_concat.append(df)\n",
    "\n",
    "merged_Gu = pd.concat(dfs_to_concat, ignore_index=True)\n",
    "\n",
    "merged_Gu.to_csv('merged_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0353e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Replace with your CSV file path\n",
    "csv_file_path = 'Gu Eileen.csv'\n",
    "\n",
    "# Read the CSV file, specifying 'str' as the dtype parameter\n",
    "df = pd.read_csv(csv_file_path, encoding='utf-8-sig', dtype={'Comment': str})\n",
    "\n",
    "# Get the column containing post comments\n",
    "post_comments = df['Comment']\n",
    "\n",
    "# Function to clean text data\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove numbers\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Remove emoji symbols\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # Symbols & Punctuation\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "                           u\"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
    "                           u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                           u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Symbols and Pictographs\n",
    "                           u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                           u\"\\U0001FA00-\\U0001FA6F\"  # Supplemental Symbols and Pictographs\n",
    "                           u\"\\U0001FA70-\\U0001FAFF\"  # Supplemental Symbols and Pictographs\n",
    "                           u\"\\U0001FB00-\\U0001FBFF\"  # Symbols for Legacy Computing\n",
    "                           u\"\\U0001F004-\\U0001F0CF\"  # Miscellaneous Symbols and Pictographs\n",
    "                           u\"\\U0001F0D0-\\U0001F0FF\"  # Miscellaneous Symbols and Pictographs\n",
    "                           u\"\\U0001F10D-\\U0001F10F\"  # Miscellaneous Symbols and Pictographs\n",
    "                           u\"\\U0001F170-\\U0001F19A\"  # Miscellaneous Symbols and Pictographs\n",
    "                           u\"\\U0001F200-\\U0001F251\"  # Miscellaneous Symbols and Pictographs\n",
    "            \"]+\", flags=re.UNICODE)\n",
    "        text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "        # Remove non-alphabetic characters and spaces, keep only Chinese characters and English\n",
    "        text = re.sub(r'[^a-zA-Z\\u4e00-\\u9fa5\\s]', '', text)\n",
    "        return text\n",
    "    else:\n",
    "        # If the data is not a string, you can return an empty string or handle it appropriately\n",
    "        return \"\"\n",
    "\n",
    "# Clean the text in all content columns\n",
    "df['Cleaned_Comment'] = df['Comment'].apply(clean_text)\n",
    "\n",
    "cleaned_csv_file_path = 'Cleaned_Gu.csv'\n",
    "df.to_csv(cleaned_csv_file_path, index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
